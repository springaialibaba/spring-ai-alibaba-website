---
title: Chat Client API
keywords: ["Spring AI", "ChatClient", "Chat Client", "Fluent API", "AI Model"]
description: "Learn how to use the Chat Client API for conversational AI interactions with Spring AI"
---

# Chat Client API

*This content is referenced from Spring AI documentation*

The ChatClient offers a fluent API for communicating with an AI Model. It supports both a synchronous and streaming programming model.

> **Note**: See the [Implementation Notes](#implementation-notes) at the bottom of this document related to the combined use of imperative and reactive programming models in ChatClient.

The fluent API has methods for building up the constituent parts of a Prompt that is passed to the AI model as input. The Prompt contains the instructional text to guide the AI model's output and behavior. From the API point of view, prompts consist of a collection of messages.

The AI model processes two main types of messages: user messages, which are direct inputs from the user, and system messages, which are generated by the system to guide the conversation.

These messages often contain placeholders that are substituted at runtime based on user input to customize the response of the AI model to the user input.

There are also Prompt options that can be specified, such as the name of the AI Model to use and the temperature setting that controls the randomness or creativity of the generated output.

## Creating a ChatClient

The ChatClient is created using a ChatClient.Builder object. You can obtain an autoconfigured ChatClient.Builder instance for any ChatModel Spring Boot autoconfiguration or create one programmatically.

### Using an autoconfigured ChatClient.Builder

In the most simple use case, Spring AI provides Spring Boot autoconfiguration, creating a prototype ChatClient.Builder bean for you to inject into your class. Here is a simple example of retrieving a String response to a simple user request.

```java
@RestController
class MyController {

    private final ChatClient chatClient;

    public MyController(ChatClient.Builder chatClientBuilder) {
        this.chatClient = chatClientBuilder.build();
    }

    @GetMapping("/ai")
    String generation(String userInput) {
        return this.chatClient.prompt()
            .user(userInput)
            .call()
            .content();
    }
}
```

In this simple example, the user input sets the contents of the user message. The `call()` method sends a request to the AI model, and the `content()` method returns the AI model's response as a String.

### Working with Multiple Chat Models

There are several scenarios where you might need to work with multiple chat models in a single application:

- Using different models for different types of tasks (e.g., a powerful model for complex reasoning and a faster, cheaper model for simpler tasks)
- Implementing fallback mechanisms when one model service is unavailable
- A/B testing different models or configurations
- Providing users with a choice of models based on their preferences
- Combining specialized models (one for code generation, another for creative content, etc.)

By default, Spring AI autoconfigures a single ChatClient.Builder bean. However, you may need to work with multiple chat models in your application. Here's how to handle this scenario:

In all cases, you need to disable the ChatClient.Builder autoconfiguration by setting the property `spring.ai.chat.client.enabled=false`.

This allows you to create multiple ChatClient instances manually.

#### Multiple ChatClients with a Single Model Type

This section covers a common use case where you need to create multiple ChatClient instances that all use the same underlying model type but with different configurations.

```java
// Create ChatClient instances programmatically
ChatModel myChatModel = ... // already autoconfigured by Spring Boot
ChatClient chatClient = ChatClient.create(myChatModel);

// Or use the builder for more control
ChatClient.Builder builder = ChatClient.builder(myChatModel);
ChatClient customChatClient = builder
    .defaultSystemPrompt("You are a helpful assistant.")
    .build();
```

#### ChatClients for Different Model Types

When working with multiple AI models, you can define separate ChatClient beans for each model:

```java
import org.springframework.ai.chat.ChatClient;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class ChatClientConfig {

    @Bean
    public ChatClient openAiChatClient(OpenAiChatModel chatModel) {
        return ChatClient.create(chatModel);
    }

    @Bean
    public ChatClient anthropicChatClient(AnthropicChatModel chatModel) {
        return ChatClient.create(chatModel);
    }
}
```

You can then inject these beans into your application components using the @Qualifier annotation:

```java
@Configuration
public class ChatClientExample {

    @Bean
    CommandLineRunner cli(
            @Qualifier("openAiChatClient") ChatClient openAiChatClient,
            @Qualifier("anthropicChatClient") ChatClient anthropicChatClient) {

        return args -> {
            var scanner = new Scanner(System.in);
            ChatClient chat;

            // Model selection
            System.out.println("\nSelect your AI model:");
            System.out.println("1. OpenAI");
            System.out.println("2. Anthropic");
            System.out.print("Enter your choice (1 or 2): ");

            String choice = scanner.nextLine().trim();

            if (choice.equals("1")) {
                chat = openAiChatClient;
                System.out.println("Using OpenAI model");
            } else {
                chat = anthropicChatClient;
                System.out.println("Using Anthropic model");
            }

            // Use the selected chat client
            System.out.print("\nEnter your question: ");
            String input = scanner.nextLine();
            String response = chat.prompt(input).call().content();
            System.out.println("ASSISTANT: " + response);

            scanner.close();
        };
    }
}
```

#### Multiple OpenAI-Compatible API Endpoints

The OpenAiApi and OpenAiChatModel classes provide a `mutate()` method that allows you to create variations of existing instances with different properties. This is particularly useful when you need to work with multiple OpenAI-compatible APIs.

```java
@Service
public class MultiModelService {

    private static final Logger logger = LoggerFactory.getLogger(MultiModelService.class);

    @Autowired
    private OpenAiChatModel baseChatModel;

    @Autowired
    private OpenAiApi baseOpenAiApi;

    public void multiClientFlow() {
        try {
            // Derive a new OpenAiApi for Groq (Llama3)
            OpenAiApi groqApi = baseOpenAiApi.mutate()
                .baseUrl("https://api.groq.com/openai")
                .apiKey(System.getenv("GROQ_API_KEY"))
                .build();

            // Derive a new OpenAiApi for OpenAI GPT-4
            OpenAiApi gpt4Api = baseOpenAiApi.mutate()
                .baseUrl("https://api.openai.com")
                .apiKey(System.getenv("OPENAI_API_KEY"))
                .build();

            // Derive a new OpenAiChatModel for Groq
            OpenAiChatModel groqModel = baseChatModel.mutate()
                .openAiApi(groqApi)
                .defaultOptions(OpenAiChatOptions.builder().model("llama3-70b-8192").temperature(0.5).build())
                .build();

            // Derive a new OpenAiChatModel for GPT-4
            OpenAiChatModel gpt4Model = baseChatModel.mutate()
                .openAiApi(gpt4Api)
                .defaultOptions(OpenAiChatOptions.builder().model("gpt-4").temperature(0.7).build())
                .build();

            // Simple prompt for both models
            String prompt = "What is the capital of France?";

            String groqResponse = ChatClient.builder(groqModel).build().prompt(prompt).call().content();
            String gpt4Response = ChatClient.builder(gpt4Model).build().prompt(prompt).call().content();

            logger.info("Groq (Llama3) response: {}", groqResponse);
            logger.info("OpenAI GPT-4 response: {}", gpt4Response);
        }
        catch (Exception e) {
            logger.error("Error in multi-client flow", e);
        }
    }
}
```

## ChatClient Fluent API

The ChatClient fluent API allows you to create a prompt in three distinct ways using an overloaded prompt method to initiate the fluent API:

- `prompt()`: This method with no arguments lets you start using the fluent API, allowing you to build up user, system, and other parts of the prompt.
- `prompt(Prompt prompt)`: This method accepts a Prompt argument, letting you pass in a Prompt instance that you have created using the Prompt's non-fluent APIs.
- `prompt(String content)`: This is a convenience method similar to the previous overload. It takes the user's text content.

## ChatClient Responses

The ChatClient API offers several ways to format the response from the AI Model using the fluent API.

### Returning a ChatResponse

The response from the AI model is a rich structure defined by the type ChatResponse. It includes metadata about how the response was generated and can also contain multiple responses, known as Generations, each with its own metadata. The metadata includes the number of tokens (each token is approximately 3/4 of a word) used to create the response. This information is important because hosted AI models charge based on the number of tokens used per request.

An example to return the ChatResponse object that contains the metadata is shown below by invoking `chatResponse()` after the `call()` method.

```java
ChatResponse chatResponse = chatClient.prompt()
    .user("Tell me a joke")
    .call()
    .chatResponse();
```

### Returning an Entity

You often want to return an entity class that is mapped from the returned String. The `entity()` method provides this functionality.

For example, given the Java record:

```java
record ActorFilms(String actor, List<String> movies) {}
```

You can easily map the AI model's output to this record using the `entity()` method, as shown below:

```java
ActorFilms actorFilms = chatClient.prompt()
    .user("Generate the filmography for a random actor.")
    .call()
    .entity(ActorFilms.class);
```

There is also an overloaded entity method with the signature `entity(ParameterizedTypeReference<T> type)` that lets you specify types such as generic Lists:

```java
List<ActorFilms> actorFilms = chatClient.prompt()
    .user("Generate the filmography of 5 movies for Tom Hanks and Bill Murray.")
    .call()
    .entity(new ParameterizedTypeReference<List<ActorFilms>>() {});
```

For streaming responses, see the [Streaming](../streaming/) documentation.

## Prompt Templates

The ChatClient fluent API lets you provide user and system text as templates with variables that are replaced at runtime.

```java
String answer = ChatClient.create(chatModel).prompt()
    .user(u -> u
            .text("Tell me the names of 5 movies whose soundtrack was composed by {composer}")
            .param("composer", "John Williams"))
    .call()
    .content();
```

Internally, the ChatClient uses the PromptTemplate class to handle the user and system text and replace the variables with the values provided at runtime relying on a given TemplateRenderer implementation. By default, Spring AI uses the StTemplateRenderer implementation, which is based on the open-source StringTemplate engine developed by Terence Parr.

Spring AI also provides a NoOpTemplateRenderer for cases where no template processing is desired.

> **Note**: The TemplateRenderer configured directly on the ChatClient (via `.templateRenderer()`) applies only to the prompt content defined directly in the ChatClient builder chain (e.g., via `.user()`, `.system()`). It does not affect templates used internally by Advisors like QuestionAnswerAdvisor, which have their own template customization mechanisms (see Custom Advisor Templates).

If you'd rather use a different template engine, you can provide a custom implementation of the TemplateRenderer interface directly to the ChatClient. You can also keep using the default StTemplateRenderer, but with a custom configuration.

For example, by default, template variables are identified by the `{}` syntax. If you're planning to include JSON in your prompt, you might want to use a different syntax to avoid conflicts with JSON syntax. For example, you can use the `<` and `>` delimiters.

```java
String answer = ChatClient.create(chatModel).prompt()
    .user(u -> u
            .text("Tell me the names of 5 movies whose soundtrack was composed by <composer>")
            .param("composer", "John Williams"))
    .templateRenderer(StTemplateRenderer.builder().startDelimiterToken('<').endDelimiterToken('>').build())
    .call()
    .content();
```

## Response Types

After specifying the `call()` method on ChatClient, there are several options for the response type:

- `String content()`: returns the String content of the response
- `ChatResponse chatResponse()`: returns the ChatResponse object that contains multiple generations and metadata about the response, including token usage information
- `ChatClientResponse chatClientResponse()`: returns a ChatClientResponse object that contains the ChatResponse object and the ChatClient execution context, giving you access to additional data used during the execution of advisors
- `entity()` methods to return Java types:
  - `entity(Class<T> type)`: used to return a specific entity type
  - `entity(ParameterizedTypeReference<T> type)`: used to return a Collection of entity types
  - `entity(StructuredOutputConverter<T> structuredOutputConverter)`: used to specify a custom converter

For streaming responses, use the `stream()` method instead of `call()`. See the [Streaming](../streaming/) documentation for details.

For information about configuring defaults and advanced configuration options, see the [Defaults and Configuration](../defaults-configuration/) documentation.

For information about using Advisors to enhance AI interactions with context, memory, and logging capabilities, see the [Advisors](../advisors/) documentation.

## Implementation Notes

The combined use of imperative and reactive programming models in ChatClient is a unique aspect of the API. Often an application will be either reactive or imperative, but not both.

> **Note**: When customizing the HTTP client interactions of a Model implementation, both the RestClient and the WebClient must be configured.

> **Important**: Due to a bug in Spring Boot 3.4, the "spring.http.client.factory=jdk" property must be set. Otherwise, it's set to "reactor" by default, which breaks certain AI workflows like the ImageModel.

- Streaming is only supported via the Reactive stack. Imperative applications must include the Reactive stack for this reason (e.g. spring-boot-starter-webflux).
- Non-streaming is only supportive via the Servlet stack. Reactive applications must include the Servlet stack for this reason (e.g. spring-boot-starter-web) and expect some calls to be blocking.
- Tool calling is imperative, leading to blocking workflows. This also results in partial/interrupted Micrometer observations (e.g. the ChatClient spans and the tool calling spans are not connected, with the first one remaining incomplete for that reason).
- The built-in advisors perform blocking operations for standards calls, and non-blocking operations for streaming calls. The Reactor Scheduler used for the advisor streaming calls can be configured via the Builder on each Advisor class.

## Next Steps

- Learn about [Prompts](../prompts/) for advanced prompt engineering
- Explore [Tool Calling](../tool-calling/) for function integration
- Check out [Multimodality](../multimodality/) for image and audio support
- Understand [Chat Memory](../chat-memory/) for conversation management
- See [Advisors](../advisors/) for enhanced AI interactions
- Review [Streaming](../streaming/) for real-time responses
- Configure [Defaults and Configuration](../defaults-configuration/) for advanced setup

